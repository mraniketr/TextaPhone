{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Emotion_Final.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mraniketr/twoModeCalling/blob/master/Emotion_Final_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjm_jnYYo6l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od0HmBH9B-KT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "d737538a-cbc5-4e0d-f15f-619ea305c728"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmrRmBxZo6mO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/COLAB DATA/text_emotion.csv')\n",
        "data = data.drop('author', axis=1)\n",
        "\n",
        "\n",
        "data = data.drop(data[data.sentiment == 'anger'].index)\n",
        "data = data.drop(data[data.sentiment == 'boredom'].index)\n",
        "data = data.drop(data[data.sentiment == 'enthusiasm'].index)\n",
        "data = data.drop(data[data.sentiment == 'empty'].index)\n",
        "data = data.drop(data[data.sentiment == 'fun'].index)\n",
        "data = data.drop(data[data.sentiment == 'relief'].index)\n",
        "data = data.drop(data[data.sentiment == 'surprise'].index)\n",
        "data = data.drop(data[data.sentiment == 'love'].index)\n",
        "data = data.drop(data[data.sentiment == 'hate'].index)\n",
        "data = data.drop(data[data.sentiment == 'worry'].index)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWJOtKdlo6md",
        "colab_type": "code",
        "outputId": "498a0257-b42c-4df0-8bec-15faa211cdaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "data['content'] = data['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "data['content'] = data['content'].str.replace('[^\\w\\s]',' ')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from textblob import Word\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "import re\n",
        "def de_repeat(text):\n",
        "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
        "    return pattern.sub(r\"\\1\\1\", text)\n",
        "#%%\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))\n",
        "\n",
        "freq = pd.Series(' '.join(data['content']).split()).value_counts()[-19000:]\n",
        "\n",
        "freq = list(freq.index)\n",
        "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "\n",
        "from sklearn import preprocessing\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y = lbl_enc.fit_transform(data.sentiment.values)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xdlbWpe-KHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer(analyzer='word')\n",
        "count_vect.fit(data['content'])\n",
        "X_train_count =  count_vect.transform(X_train)\n",
        "X_val_count =  count_vect.transform(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8A4Nd7o6ml",
        "colab_type": "code",
        "outputId": "c8f023c8-0172-464d-e776-79c8f15fb906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Model 3: Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=1)\n",
        "logreg.fit(X_train_count, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_val_count)\n",
        "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(logreg, open(filename, 'wb'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log reg count vectors accuracy 0.5988433228180863\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro0BuxVPo6mt",
        "colab_type": "code",
        "outputId": "bde42677-3831-47ee-de60-715a0e21f9f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "filename = 'finalized_model.sav'\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "tweets = pd.DataFrame(['I am very happy today! The atmosphere looks cheerful',\n",
        "'Things are looking great. It was such a good day',\n",
        "'Success is right around the corner. Lets celebrate this victory',\n",
        "'Martha was angry, certainly at the perpetrator but also at the Warwick police for not summarily arresting the man and rescuing the boy.',\n",
        "'Now this is my worst, okay? But I am gonna get better.',\n",
        "'I am tired, boss. Tired of being on the road, lonely as a sparrow in the rain. I am tired of all the pain I feel',\n",
        "'Im happy as hell',\n",
        "'His death broke my heart. It was a sad day'])\n",
        "# Doing some preprocessing on these tweets as done before\n",
        "tweets[0] = tweets[0].str.replace('[^\\w\\s]',' ')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "tweets[0] = tweets[0].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "from textblob import Word\n",
        "tweets[0] = tweets[0].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "# Extracting Count Vectors feature from our tweets\n",
        "tweet_count = count_vect.transform(tweets[0])\n",
        "# count_vect.fit(data['content'])\n",
        "\n",
        "#Predicting the emotion of the tweet using our already trained linear SVM\n",
        "# tweet_pred = logreg.predict(tweet_count)\n",
        "\n",
        "\n",
        "tweet_pred = loaded_model.predict(tweet_count)\n",
        "# print(result)\n",
        "\n",
        "\n",
        "print(tweet_pred)\n",
        "for i in range(0,len(tweet_pred)):\n",
        "  if tweet_pred[i]==0:\n",
        "    print(\"Happy\")\n",
        "  elif tweet_pred[i]==1:\n",
        "    print(\"Neutral\")\n",
        "  else:\n",
        "    print(\"Sad\")\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 2 1 2 1 2]\n",
            "Happy\n",
            "Happy\n",
            "Happy\n",
            "Sad\n",
            "Neutral\n",
            "Sad\n",
            "Neutral\n",
            "Sad\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}